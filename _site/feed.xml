<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-20T18:22:04+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Matthew Marsh</title><subtitle>personal website and blog</subtitle><entry><title type="html">Automating FPL with Machine Learning and MPC - Part 2: Optimisation and Control</title><link href="http://localhost:4000/blog/2025/09/15/AI-Fantasy-Football-2.html" rel="alternate" type="text/html" title="Automating FPL with Machine Learning and MPC - Part 2: Optimisation and Control" /><published>2025-09-15T13:00:00+01:00</published><updated>2025-09-15T13:00:00+01:00</updated><id>http://localhost:4000/blog/2025/09/15/AI-Fantasy-Football-2</id><content type="html" xml:base="http://localhost:4000/blog/2025/09/15/AI-Fantasy-Football-2.html"><![CDATA[<p style="text-align: center;">
<img src="/assets/images/fpl-ml-2.jpg" alt="My Photo" style="width:60%; border-radius:12px; margin: 20px 0;" />
</p>

<p>Welcome back to part 2 of the blog post on how I am automating Fantasy Football this year. This probably marks the final technical blog post on my project, I will probably further update the blog further in the season, where we review some of the model’s performance - so far there has been some interesting choices (a weird obsession with Bowen and Gundogan), and I’ll try and explain the why, as well as the what with the progression of the season.</p>

<p>Without further ado, here is how we utilised the model we built in part 1 within a control framework. This will probably get a little bit maths heavy, purely because I feel there are less explanations on the internet as to how to formulate these optimisation problems, whereas every man and his dog has done an explanation on how LSTMs work.</p>

<hr />

<h4 id="preventing-a-greedy-control-strategy"><strong>Preventing a Greedy Control Strategy</strong></h4>

<hr />

<p>In the first blog post, we briefly discussed training the model for longer time horizon predictions, to avoid taking ‘greedy’ options when utilising the control strategy. A greedy strategy would take the option that maximises expected points for that particular week. However fantasy football isn’t won in just one week (I hope, as I say sat rooted at the bottom of all the leagues I am in currently).</p>

<p>We want our model to plan for the future over different game weeks. For example, if we had the choice between Bruno Fernandes and Martin Odegaard, who are similar players, with similar prices and over the course of a season might score similar points. If Bruno’s next fixtures were Sunderland (who you’d <em>imagine</em> Man Utd might beat), Man City and Everton - one easy game and two tough games, his predicted xP might be 8.32 from the first game, but then only a measly 1.32 and then -1.41 from the next two games (probably due Jordan Pickford having that blokes number from the penalty spot). Then if Odegaard was to face Burnley, Leeds and Wolves, where he is predicted xPs of 6.43, 7.34 and 4.56; Odegaard’s xP over the <em>horizon</em> is then the more attractive pick over Fernandes. However, there is no free lunch here: the further in the future we predict, the more complex and less accurate the model is with our current data.</p>

<hr />

<h4 id="the-initial-optimisation-problem"><strong>The Initial Optimisation Problem</strong></h4>

<hr />

<p>Picking our starting FPL squad is a relaxed optimisation problem compared to the receding horizon control problem (i.e. we have less constraints). This is because we don’t have any dynamic decisions to make, so the static optimisation doesn’t have to account for transition and decision making constraints.</p>

<p>We first introduce a binary decision variable, $y_{i}$ - this is if 1 if player $i$ is picked, and 0 otherwise. Here $i$ is an index that covers the set of all players in the database which we will denote as $I$, so $i$ could be Cole Palmer, Harrison Armstrong or even Luis Diaz, who is for some reason still kicking about on the database.</p>

<p>This then allows us to define our objective function over the set of $I$ players:</p>

\[\max \sum_{i \in I} y_{i} c_{i} (\mu_{i, \text{xP}} - r \sigma_{i, \text{xP}})\]

<p>Essentially, we sum over all the players, multiply each player by $c_{i}$, which is their chance of playing - this comes from the API and accounts for injuries, suspensions or if they are fuming they haven’t been granted a transfer (a là Wissa or Isak), and then multiply this by their mean xP, $\mu_{xP}$, whilst accounting for the variance of their predicted points $\sigma_{xP}$, to some perceived risk level $r$. In this problem I set $r$ to 0.5, but the higher the value of $r$, the less risk, as the more punished a higher variance in the xP is. The optimiser here essentially ‘switches on’ certain players, and evaluates the overall expression, and then decides which players to pick based on the value when they are switched on.</p>

<p>We then have to satisfy constraints within our fantasy picks:</p>

<p>Firstly, we have to pick exactly 15 players in the squad, so:</p>

\[\sum_i^N y_i = 15\]

<p>We also have constraints on the numbers of positions for each player. Firstly, we define the set of positions, 
$P = { GK, DEF, MID, FW }$.</p>

<p>We can then define the subset of each player for each position, 
$I_p = { i \in I : \text{pos}(i) = p }$.</p>

<p>For example, Marc Guehi would belong to $I_{\text{DEF}}$.</p>

<p>Then, we have equality constraints for each position:
$L_{GK} = 2, \; L_{DEF} = 5, \; L_{MID} = 5, \; L_{FW} = 3$</p>

<p>Finally, we can define the constraint over the set of positions, whereas before it was defined over all players:</p>

\[\sum_{i \in I_p} y_i = L_p \quad \forall p \in P\]

<p>Similarly, we can define the set of clubs, 
$C = { \text{Everton}, \text{Arsenal}, \cdots }$. 
Again, we have the subset of players belonging to each team, $I_c = { i \in I : \text{club}(i) = c }$.</p>

<p>As I write this, Marc Guehi <em>currently</em> belongs to $I_{\text{Crystal Palace}}$.</p>

<p>As we can’t have more than 3 players from the same club, this is an inequality constraint:
\(\sum_{i \in I_c} y_i \leq 3 \quad \forall c \in C\)</p>

<p>We also have a constraint on the budget, which we define as a further inequality constraint:</p>

\[\sum_{i \in I} y_i v_i \leq 100\]

<p>Here, $v_i$ is the value of player $i$.</p>

<p>When solving these optimisation problems, inequality constraints can either be <em>active</em> or <em>inactive</em>. Active means they are satisfied by equality, i.e. the cost of the squad is exactly 100, or we have exactly 3 Arsenal players in the squad. 
The full optimisation problem can then be written as:</p>

\[\begin{aligned}
\max_{y_i \in \{0,1\}} \quad &amp; \sum_{i \in I} y_i \, c_i \left( \mu_{i,\text{xP}} - r \, \sigma_{i,\text{xP}} \right) \\[1em]
\text{s.t.} \quad 
&amp; \sum_{i \in I} y_i = 15 
&amp;&amp; \text{(squad size)} \\[0.5em]
&amp; \sum_{i \in I_p} y_i = L_p \quad \forall p \in P 
&amp;&amp; \text{(position limits: }L_{GK}=2,\ L_{DEF}=5,\ L_{MID}=5,\ L_{FW}=3\text{)} \\[0.5em]
&amp; \sum_{i \in I_c} y_i \leq 3 \quad \forall c \in C
&amp;&amp; \text{(team limit)} \\[0.5em]
&amp; \sum_{i \in I} y_i v_i \leq 100
&amp;&amp; \text{(budget constraint)}
\end{aligned}\]

<p>This is all linear in our decision variables across the objective function and constraints, meaning this is an Integer Linear Program. This is a convex optimisation problem, which means all the solutions are globally optimal - i.e. our solver won’t get stuck trying to pick substandard players!</p>

<p>We ended up with an initial squad of:</p>

<ol>
  <li>Jordan Pickford — Everton — Goalkeeper — £5.5</li>
  <li>Ezri Konsa  — Aston Villa — Defender — £4.5</li>
  <li>Rúben Dias — Man City — Defender — £5.5</li>
  <li>Aaron Wan-Bissaka — West Ham — Defender — £4.5</li>
  <li>Dan Burn — Newcastle — Defender — £5.0</li>
  <li>Bruno Fernandes — Man Utd — Midfielder — £9.0</li>
  <li>Bryan Mbeumo — Man Utd — Midfielder — £8.0</li>
  <li>Mohamed Salah — Liverpool — Midfielder — £14.5</li>
  <li>Sandro Tonali — Newcastle — Midfielder — £5.5</li>
  <li>Jarrod Bowen — West Ham — Forward — £8.0</li>
  <li>Raúl Jiménez — Fulham — Forward — £6.5</li>
  <li>Nick Pope — Newcastle — Goalkeeper — £5.0</li>
  <li>İlkay Gündoğan — Man City — Midfielder — £6.4</li>
  <li>Reece James — Chelsea — Defender — £5.5</li>
  <li>Jørgen Strand Larsen — Wolves — Forward — £6.5</li>
</ol>

<p>From this, we just picked the player with the highest predicted xP to be the captain (Bowen?!?!?!?), and then the top 11 players that fit into a squad as the starting XI. This seemed ok. Not what I would have chosen as my first choice, but my hypotheses for why this isn’t perfect will come, when I have given the model more of a chance!</p>

<hr />

<h4 id="receding-horizon-model-predictive-control"><strong>Receding Horizon Model Predictive Control</strong></h4>

<hr />

<p>Now we have an initial starting point, I then made the optimisation dynamic. Receding horizon predictive control is one type of MPC strategy. Here, we have a model (our ML model), which will predict $T$ timesteps into the future, and we use an optimisation problem to decide which decisions to make to maximise an objective function over that time horizon, subject to constraints on that horizon. We then re-solve this problem at every time step, where we make updates to the control strategy; whilst attempting to plan-ahead.</p>

<p>When we make this dynamic, we have a few more decision variables - not just the players to pick. Firstly, our sets are not just defined over the set of players, $I$; but also the set of time horizons: $\mathcal{T} \in {t, \cdots,  T}$ - then the previous variable of the player being in the squad is also defined over time - i.e. $y_{i,t}$</p>

<p>We then add more binary decision variables:</p>
<ul>
  <li>$z_{i,t}$, this is 1 if player $i$ is in the starting XI at time $t$, and 0 otherwise.</li>
  <li>$c_{i,t}$, which is 1 if player $i$ is the captain at time $t$, and 0 otherwise.</li>
  <li>$s_{i,t}$, this is 1 if player $i$ has been sold at time $t$, and 0 otherwise.</li>
  <li>$b_{i,t}$, this is 1 if player $i$ has been bought at time $t$, and 0 otherwise.</li>
</ul>

<p>This allows us to define our objective function, just with a summation over the time horizon as well (accounting for also taking a points hit for transfers):</p>

\[\max \sum_{t \in T} \sum_{i \in I} y_{i} c_{i} (\mu_{i,t, \text{xP}} - r \sigma_{i,t, \text{xP}}) - 4*p[t]\]

<p>Unfortunately, this means we now need to add loads of constraints to ensure the model does what we want it to do.</p>

<p>Firstly, the initial squad rule - i.e. at time 0, the indices of the selected players need to be the ones in our squad:</p>

<p>$ y_{i, 0} = 1$ if player $i$ is in the squad, 0 otherwise</p>

<p>We then need to define state transition rules - i.e. ensure there is continuity between the players depending on the actions to take each week.
Firstly, the squad evolution constraint - i.e. the players in the squad at time t, must be the players in the squad at time t-1, minus the ones we sold and plus the ones we have bought:</p>

\[y_{i, t} = y_{i, t-1} - s_{i,t} + b_{i, t}, \quad t&gt;0\]

<p>The case at $t = 0$ is covered by the initial rule.</p>

<p>Then the budget evolution rule - i.e your bank balance is the balance prior accounting for transfers:</p>

\[\text{bank}_t = \text{bank}_{t-1} + v_i s_{i,t} - v_i b_{i,t}\]

<p>Then have constraints on the transfers made:</p>

\[\text{transfers_made}t = \sum{i \in I} b_{i,t}, \quad \forall t \in \mathcal{T}\]

<p>With logic to try and reduce number of paid transfers:</p>

\[\text{paid_transfers}_t \geq \text{transfers_made}_t - \text{free_transfers}_t, \quad \forall t \in \mathcal{T}\]

<p>Constraint on the evolution of free transfers too</p>

\[\text{free_transfers}t = 1 + \text{free_transfers}{t-1} - \text{transfers_made}_{t-1}, \quad t &gt; 0, \quad \text{with } \text{free_transfers}_t \leq 2\]

<p>You have to buy and sell the same number of players - i.e. you can’t just buy one player and go over the squad limit.</p>

\[\sum_{i \in I} s_{i,t} = \sum_{i \in I} b_{i,t}, \quad \forall t \in \mathcal{T}\]

<p>Then more logic regarding transfers, i.e. you cannot buy a player who is already in your squad:</p>

\[b_{i,t} + y_{i,t-1} \leq 1, \quad \forall i \in I, ; t &gt; 0\]

<p>and can only sell players you currently own:</p>

\[s_{i,t} \leq y_{i,t-1}, \quad \forall i \in I, ; t &gt; 0\]

<p>Finally, the same constraints as the static problem are translated to the dynamic setting. At any time step, the squad must contain exactly 15 players:</p>

\[\sum_{i \in I} y_{i,t} = 15, \quad \forall t \in \mathcal{T}\]

<p>with the correct positions:</p>

\[\sum_{i \in I_{\text{GK}}} y_{i,t} = 2, \quad
\sum_{i \in I_{\text{DEF}}} y_{i,t} = 5, \quad
\sum_{i \in I_{\text{MID}}} y_{i,t} = 5, \quad
\sum_{i \in I_{\text{FWD}}} y_{i,t} = 3\]

<p>and a team limit:</p>

\[\sum_{i \in I_{k}} y_{i,t} \leq 3, \quad \forall k \in \text{Teams}, ; \forall t \in \mathcal{T}\]

<p>As our model makes decisions on the captains and starting XI, we need to also define this. The starting XI must come from players within the squad:</p>

\[z_{i,t} \leq y_{i,t}, \quad \forall i \in I, ; t \in \mathcal{T}\]

<p>Must contain exactly 11 players:</p>

\[\sum_{i \in I} z_{i,t} = 11, \quad \forall t \in \mathcal{T}\]

<p>and again requires positions:</p>

\[\sum_{i \in I_{\text{GK}}} z_{i,t} = 1, \quad
\sum_{i \in I_{\text{DEF}}} z_{i,t} \geq 3, \quad
\sum_{i \in I_{\text{FWD}}} z_{i,t} \geq 1\]

<p>The captain must be in the starting XI:</p>

\[c_{i,t} \leq z_{i,t}, \quad \forall i \in I, ; t \in \mathcal{T}\]

<p>and can only pick one!</p>

\[\sum_{i \in I} c_{i,t} = 1, \quad \forall t \in \mathcal{T}\]

<p>This had lead us to a long final time-dependent optimisation problem - that I will not be retyping out. We solve this at every time step, to decide the transfers, starting XI and captain for the next week.</p>

<hr />

<h4 id="will-this-be-any-good"><strong>Will this be any good?</strong></h4>

<hr />

<p>So finally, do I think all the hours of time I put into building the model, control strategy and writing up how I did it will pay dividends in terms of beating everyone?
Short answer: probably not.</p>

<p>To quote a visiting student in our group, Joao, “MPC is only as good as your model” - and he’s right.</p>

<p>Currently the model isn’t great. It doesn’t have too much data, and is pretty narrow minded in terms of its picks. Although I incorporated some pretty basic uncertainty, it is pretty much a deterministic model. When it is translating how good a given player <em>might be</em> in the Premier League, it just picks a random value. It would be better to make almost everything stochastic - and with that, it would require a lot more data, and a lot more compute. I will probably devote some more time to trying to do this - but I also have to evade all the bot detections; but it will be interesting to see how good I can get this - and maybe one day, I’ll be using it to make money on BJ88.html, or one of those other totally betting sites that sponsor Premier League teams (and probably have terribly priced odds!).</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Automating FPL with Machine Learning and MPC - Part 1: Machine Learning</title><link href="http://localhost:4000/blog/2025/08/21/AI-Fantasy-Football-1.html" rel="alternate" type="text/html" title="Automating FPL with Machine Learning and MPC - Part 1: Machine Learning" /><published>2025-08-21T13:00:00+01:00</published><updated>2025-08-21T13:00:00+01:00</updated><id>http://localhost:4000/blog/2025/08/21/AI-Fantasy-Football-1</id><content type="html" xml:base="http://localhost:4000/blog/2025/08/21/AI-Fantasy-Football-1.html"><![CDATA[<p style="text-align: center;">
<img src="/assets/images/fpl-ml-1.jpg" alt="My Photo" style="width:60%; border-radius:12px; margin: 20px 0;" />
</p>

<p>I like football. I watch Everton disappoint me every week and mainly hatewatch Liverpool as well. But I can never be bothered with fantasy football. I have no interest in watching Brentford v Fulham; or checking whether Ben Gannon-Doak’s knee injury is worth him going on my bench or substituting for someone else. I don’t care to check whether the xG overperformance of Strand-Larsen is worth his discounted price compared to Chris Wood; and I definitely can’t be bothered to check this every week for 38 weeks. Excluding double game weeks, or the festive period, or if we decide to put a world cup in the middle of winter or whatever. But what I lack in attention span, I make up for with short term obsession; so I decided it would be an interesting project to try and get a computer to do all this for me instead.</p>

<hr />

<h4 id="a-discussion-on-data"><strong>A Discussion on Data</strong></h4>

<hr />

<p>The most painful part of the project was getting the data. I argued a lot with ChatGPT about the best way to scrape all the stats from <a href="https://fbref.com/en/">FBref</a>, and this got really complex having to pretend I wasn’t a bot, and then deciding which stats were worth scraping, which were revelant (progressive carries?!) and which were actually worth using up precious space. Whilst I enjoy the more mathematical side of football, there really is some nuance in deciding if a statistic is really relevant; or as I like to think of it, predictive. Knowing the goal probability of a given shot (xG) is really powerful, and then being able to condition that probability on factors like keeper position and number of defenders in the way (xGOT) as well gives an idea of the quality of a given action. But knowing the amount of touches Everton had in the opposition penalty area the first half against Leeds (that would be zero) merely confirms my eyes telling me they were garbage. It doesn’t allow us to reproduce the actions, and from that evaluate <em>how likely</em> the match outcome was <em>to happen</em>. A better metric would be the probability of scoring from getting to these areas (or expected threat, xT, which would also have been near zero).</p>

<p>The overstatification of the Premier League is becoming more apparent, and we have shifted from subjective garbage, like Michael Owen speculating “the team who scores more usually wins…” to quantitivate, but irrelevant information, like Darren Fletcher telling us Man City have had the most corners in the Premier League this year. Brilliant value add. There’s a really good introduction to xG as a predictive stat in this <a href="https://www.amazon.co.uk/Expected-Goals-Philosophy-Game-Changing-Analysing/dp/1089883188">book</a>, and then a great discussion how it can actually be used in practise <a href="https://www.amazon.co.uk/How-Win-Premier-League-Bestselling/dp/1804950300/ref=pd_lpo_d_sccl_3/260-6989266-6862468?pd_rd_w=9QF3S&amp;content-id=amzn1.sym.bb13d3fc-af40-4fff-a822-e0e4c415da96&amp;pf_rd_p=bb13d3fc-af40-4fff-a822-e0e4c415da96&amp;pf_rd_r=ZA13Z83ZMYRB60BPNH1R&amp;pd_rd_wg=IPk2b&amp;pd_rd_r=073a2bb6-f156-481f-9769-9a00657f6f76&amp;pd_rd_i=1804950300&amp;psc=1">here</a>, even if it’s about the most insufferable club in the world.</p>

<p>Anyway, I spent a lot of time trying to sort this, until I discovered that someone has pulled all the FPL data, and with that a lot of stats from the Premier League using the API each week. I was glad I wasted all my time when I could have used the GitHub repo <a href="https://github.com/vaastav/Fantasy-Premier-League">here</a>, so I ended up using it anyway.</p>

<hr />

<h4 id="using-machine-learning"><strong>Using Machine Learning</strong></h4>

<hr />

<p>So now we have the data, let’s see if my PhD research is any use, and see if we can predict the future. The repo contains all the players in each gameweek in a given season, with relevant years being 2021/22 onwards (because of the integration of predictive stats). Each week contains stats on:</p>
<ul>
  <li>Minutes and Starts</li>
  <li>Classic Stats like: Goals, Assists, Clean Sheets, Goals Conceded, Own Goals, Penalties Missed/Scored, Red/Yellow Cards etc.</li>
  <li>Expectations: xG, xA, xGI, xGC</li>
  <li>FPL Proprietary Stats: Threat, Influence, Creativity, ICT Index; and Bonus Points.</li>
</ul>

<p>These are then integrated alongside the Expected Points (xP) for each gameweek; where we look more at the <em>predictive</em> side of the game, which hopefully over the course of the season would even itself out, instead of getting lucky at Chris Wood outperforming his xG each week.</p>

<p><strong>The Predictive Model</strong></p>

<p>The philosophy behind the model is a number of points a player will score is a function of their <em>form</em> and the team <em>matchup</em>.
For example, a good attacking player is more likely to score against a poor, and hence score points, than against the great wall of James Tarkowski and Jarrad Branthwaite; but conversely those two beasts are more likely to be scoring when facing a toothless attacker. The PL API has a reasonable metric for replicating the <em>strength</em> of given team’s home and away attack and defence strength (as statistically, there is a home advantage of something like 7%), and we were able to utilise this to train an interesting ML model.</p>

<p>I settled on using a bi-input LSTM/MLP model. An Multilayer Perceptron or Artificial or Feedforward Neural Network is standard sequential neural net. It’s a load of linear layers that use an activation function to add non-linearity which is then able to capture complex relationships between input and output variables (like how expected points and a team’s strength are related!). An LSTM is a variant of Recurrent Neural Network. It is good because it can work better with sequential data, like a how players stats for previous matches might impact their expected points, whilst being able to store previous hidden states, to act as ‘memory’. They use magic ‘gates’ to pass data through, which use different activation functions to prevent exploding/vanishing gradients during loss calculations.</p>

<p>This LSTM, however, not only has its magic hidden, input and forget gates, but also utilises an attention mechanism, to ‘enhance its ability to learn sequential dependencies’. Attention is the backbone of large language models and is essentially the calculation that figures out to say ‘you’re welcome’ when you thank ChatGPT for its help. It relates the importance of different datapoints in a sequence, like how relevant is a players xG in the previous game to one 5 games ago to predicting their xP in the upcoming fixture. Or how relevant the word relevant is in this sentence, to predict that the next word is apple.</p>

<p>Either way, this whole cell then takes in each players previous 5 gameweek form, taking in their prior performance in stats, and transforms this to an ‘embedded latent vector’. I used an attention mechanism as my initial training of the model was giving <em>terrible results</em>, and this was the first thing Gemini, Claude and ChatGPT suggested doing as it would ‘improve my model’. A schematic of the model structure is shown below:</p>

<p style="text-align: center;">
<img src="/assets/images/fpl_model.png" alt="My Photo" style="width:100%; border-radius:12px; margin: 20px 0;" />
</p>

<p>In actual fact, trying to train on a dataset with loads of outliers on the MSE loss function doesn’t give great results. It’s actually much better to use the MAE or HuberLoss (adapts between MSE and MAE), because the xP is quite erratic. Perhaps I should have tried to visualise this beforehand and saved myself all this time, but this can be the first learning curve of the project.</p>

<p>I chose 5 previous games because it was a nice round number, and as I started the project about 4 days before the FPL deadline, so I didn’t have time to mess with it. The MLP takes the next 3 teams the player is going to face, and their ‘strength ratings’, relative to the original team. The combination of both these input heads, creates a final embedded vector that is then passed through an MLP, to predict the xP for the players’ next 3 games. The model predicts 3 future games as a trade-off between taking a greedy strategy (TBD in part 2!), whilst also maintaining a fairly accurate predictive model. Again, I would try and tune this if I were to rebuild this model. This was also simpler than predicting the next gameweek stats and iteratively predicting future gameweeks, which would make my controller worse. More on this later.</p>

<p>I ended up training these models with the HuberLoss function, with 2 input MLP layers, 2 LSTM layers, with each layer containing 128 neurons. I trained the model on all players within the 2021-22, 2022-23, 2023-24 seasons. I did enjoy the model breaking at GW7 in 2022-23 (once in a generation moment as to why). My test set was the most recent 2024-25 season. As we are utilising temporal data, it’s important to filter the data like this to avoid data-leakage and making the model crap. This gave me a loss of about 0.12 across the train and test set, which although isn’t perfect is probably fine, because firstly, I didn’t really have the time to improve this or scrape more / better quality data. Secondly, although an MPC is only as good as your model, this is in essence a preferential learning problem. My model might predict Erling Haaland to get 6.87 xP this week, and he might end up with 8.35 xP instead; but as long as the model knew Haaland is a better pick than Jack Harrison, it isn’t that deep. So to quote one of my undergrad lecturers, here we found that ‘if it works, it’s good enough’.</p>

<p>One of my biggest hates when I read papers that use ‘ML’ is the lack of uncertainty quantification in the models that are used. Loads of research treats these models as magic that will predict everything with ease, and without actually understanding their limitations (which like the predictive stats, is important to know). Training a neural network is so stochastic what is to stop it predicting that Harrison Armstrong is going to get 10.47 points this weekend, there’s no guarantees that your loss is in any way correct. But understanding how confident your model is in that prediction is one way to be certain about your uncertainty.</p>

<p>I was at a summer school recently, where one talk was attempting to bridge the gap between statistics and ML communities, especially the theory grounding uncertainty quantification. If there’s one thing I took away, was his inner hatred of ensembling as a method of UQ. “It’s not Bayesian in the slightest”, “it’s not acceptable within safety critical applications at all”. This application is probably not too safety critical, and ensembles are quite an easy way to get a value for uncertainty in a prediction. After all, what would be a reasonable ‘prior’ to assume on weights to predict football matches. Either way, DeepMind’s weather models do it, and if it’s good enough for them, it is after all ‘better than nothing’, so Jeremias if by chance you are reading this. Sorry. Did like the talk though was interesting. And it figured out there was a massive variance when the single model was trying to pick Gvardiol as captain every week.</p>

<p>I ended up training 10 models in parallel to predict the xP for each player, which allowed for more consistency in predictions, and reduces downside risk. If we take two players for example:
Erling Haaland, gets predicted xPs by our ensemble of 8.35, 7.92, 8.12, 9.14, etc, which gives you a mean of about 8 something and a standard deviation of around 1. I am not sure because I made them up.</p>

<p>But then, say our model wants to predict Michael Keane’s xP, which the ensemble works out as 1.27 (realistic, probably gave away a penalty), 14.37 (he probably scored a worldie), 3.46, 0.45, etc.</p>

<p>If our singular model just predicted that MK would get 14.37 xP compared to Haaland’s 7.92, our controller would then take the Keggerman, which sorry Michael, we probably wouldn’t want to do, and gives the intuition behind ensembling multiple models.</p>

<hr />
<h4 id="potential-improvements"><strong>Potential Improvements</strong></h4>
<hr />

<p>The first issue with the model lies within the data itself. Lots of the features the model relies on for prediction are proprietary, such as team strength ratings, threat, creativity, ICT index, etc. Although there is a rough indication what they mean, actually understanding where these values come from could allow the model to be adjusted to enhance predictive accuracy, or built in a better way, as these probably aren’t optimised to predict their points, more just a way to gauge a player or team’s ability.</p>

<p>As these stats are proprietary to the Premier League, there is no clear of getting a players creativity in a different league. For example, trying to figure out the threat rating Gyokeres should be assigned is no easy feat, we can either ignore players who have joined the league, and potentially miss out on loads of points, like with Haaland, or try and find a proxy.</p>

<p>To try and incorporate players who are new to the Premier League, firstly I scraped all the players who had joined the Premier League since 2020 from all major leagues. From this, I found a multiplier that corresponds to stats in one league to the next, which was some horrible distribution. For example, to translate xG/90 from the Portuguese League to the Premier League, you multiply by around 0.78, meaning a certain goal in the Premier League might only be scored 3/4 times in Portugal. Of course, this is a crude approximation, but I’d argue this is better than assuming all leagues are the same standard (they’re not).</p>

<p>After applying these factors, I also built a standard 3 layer Neural Network to serve as a proxy to predict a players creativity, threat and ICT index, given various stats about their xG, xGOT, xA, as well as their actual goals and assists. This essentially allowed us to create a proxy for Viktor Gyokeres’s form had he played in the Premier League, going into this season. However, had I had more time, I’d probably take a more probabilistic approach, and using Monte-Carlo simulations, could have given a better estimate of this, rather than directly translating each player using average values.</p>

<p>Developing my own player ratings and predictive metrics could be one route to improve the predictive accuracy, however this would be fundamentally limited by data. There are concepts, such as conditional expected threat, where the idea is to take a player such as Illiman Ndiaye, and work out how many goals would he assist, if he was to be playing the ball through to Haaland, rather than Dominic Calvert-Lewin, who is usually too busy tripping over his high heels to put the ball in the goal. However, even with all of the scraping in the world, there just isn’t enough public access to high quality data to build a model like this (collab @opta?).</p>

<hr />

<p>I think this just about concludes the first part into what could be 2, could be a 3 part series on how I am trying to lose mates using ML and Fantasy Football. It might have been waffley, and in the future I might look back and think it was all badly written and useless, but lets see how well the model does this year. The next parts, I’ll talk about applying this model within a Mixed Integer Linear Programming (MILP) framework to firstly build a static ‘optimised’ FPL team, and then how I developed it to use a full receding horizon control strategy, to dynamically make decisions on starting XI, captains and transfers for a full season.</p>

<p>A big shoutout to Vaastav on the <a href="https://github.com/vaastav/Fantasy-Premier-League">GitHub repo</a>, it’s been a fantastic help with this project. Also big thanks to Iwan Pavord, who’s one of my colleagues in our research group. He somewhat inspired me to look into this when he first joined our group, and he did something cool about trying to track your own minileagues between mates. He’s also trying to do a similar thing but using Reinforcement Learning instead, which is way too much compute/building my own environment to bother getting involved with. I’ll leave a link to his project if he writes it up.</p>

<p>Hope you have enjoyed reading, make sure to like, subscribe, leave a comment below and I will see you on the next one.</p>

<p>Don’t go changin’</p>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Helllloooo</title><link href="http://localhost:4000/blog/2025/08/18/first-post.html" rel="alternate" type="text/html" title="Helllloooo" /><published>2025-08-18T13:00:00+01:00</published><updated>2025-08-18T13:00:00+01:00</updated><id>http://localhost:4000/blog/2025/08/18/first-post</id><content type="html" xml:base="http://localhost:4000/blog/2025/08/18/first-post.html"><![CDATA[<p>Welcome to my website and blog!</p>

<p>I plan on using this website to document some (hopefully) interesting side projects I’ll work on alongside my PhD (and beyond), that one day might help me achieve gainful employment. I’ll also be shamelessly promoting whatever (if any) research I end up producing, as well as any generic thoughts/waffle I decide to write down, probably around ML, finance, football, cycling, hockey etc.</p>

<p>Maybe one day I’ll look back at how creative and keen I was as a first year PhD, but I imagine more likely will see how horrible my coding practise was and how it doesn’t matter anymore as LLMs do everything now anyway.</p>

<p>If anything I have written about seems interesting, feel free:</p>

<ul>
  <li>to email me <a href="mailto:matthew.marsh20@imperial.ac.uk">matthew.marsh20@imperial.ac.uk</a></li>
  <li>or add my <a href="https://www.linkedin.com/in/matthew--marsh/">linkedin</a></li>
  <li>or follow my <a href="https://x.com/matthew_marsh11">twitter</a>
(or X or whatever it is called), where I mainly just repost about how depressing it is to support Everton.</li>
</ul>]]></content><author><name></name></author><category term="blog" /><summary type="html"><![CDATA[Welcome to my website and blog!]]></summary></entry></feed>